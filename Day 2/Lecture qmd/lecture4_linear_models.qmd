---
title: "lecture4_linear_models"
format: pptx
editor: visual
---

## Linear models

Linear models are everywhere in statistics

Used for:

-   Comparison

-   Prediction

------------------------------------------------------------------------

# Linear models

$$
y=a+b*x+error
$$

$y$ = response, dependent variable

$a$ and $b$ are parameters

$x$ = predictor, independent variable

$error$ = normally distributed $N(0, \sigma^2)$

------------------------------------------------------------------------

# Linear models

Can be written in multiple ways:

$$
y_i=a+b*x+\epsilon_I
$$

$$
y_i \sim N(a + b * x_I,\sigma^2)
$$

$$
y \sim N(a+b*x, \sigma^2)
$$

$$
y\sim N(X\beta, \sigma^2 I)
$$

$$
response \sim a + predictor1 + predictor2
$$

------------------------------------------------------------------------

# Covariates, predictors, independent variables

Can be continuous

Can be categorical

Multiple covariates: $y \sim a + b1*x1 + b2 * x2$

Interactions: $y \sim a + b1*x1 + b2 * x2 + b3 * x1 * x2$

Nonlinear: $\sqrt{y} \sim a + b * x^2$

------------------------------------------------------------------------

# Linear models In R (1)

We can use the `lm` function in base R

```{r, echo = TRUE, eval = FALSE}
mod = lm(response ~ predictor, data = mydata)
```

$a$ (the intercept) is assumed unless you specify `-1` in the `lm` function.

------------------------------------------------------------------------

# Fitting a regression in R (1)

$$
y \sim x
$$

$$
y \sim a + b*x
$$

```{r, echo = TRUE}
set.seed(123)
n <- 40
x <- runif(n, 0, 20)
y <- 0.2 + 0.3 * x + rnorm(n, mean = 0, sd = 0.5) 

mod <- lm(y ~ x)
```

------------------------------------------------------------------------

# Using `summary(mod)`

```{r}
summary(mod)
```

------------------------------------------------------------------------

# Displaying results (1)

```{r, echo = TRUE}
plot(x, y)
abline(mod)
```

------------------------------------------------------------------------

# Linear models in R (2)

We can also specify out own function

```{r, echo = TRUE}
lm_fun <- function(pars, y, x){
  alpha <- pars[1]
  beta <- pars[2]
  sigma <- exp(pars[3]) # Exp to keep positive
  
  nll = -sum(dnorm(y, alpha + beta * x, sigma, log = TRUE))
  return(nll)
}

fit <- optim(par = c(0,0,0), lm_fun, y = y, x = x, hessian = TRUE)
fit$par
```

------------------------------------------------------------------------

# Linear models in R (2)

Smart way using model matrix

```{r, echo = TRUE}
lm_fun <- function(pars, y, x){
  beta <- pars[1:ncol(x)]
  sigma <- exp(pars[ncol(x) + 1]) # Exp to keep positive
  
  nll = -sum(dnorm(y, x %*% beta, sigma, log = TRUE))
  return(nll)
}

x_mat <- model.matrix(formula(~ 1 + x))
fit <- optim(par = c(0,0,0), lm_fun, y = y, x = x_mat, hessian = TRUE)
fit$par
```

------------------------------------------------------------------------

# Summary (2)

We can get the variance-covariance of the parameters by inverting the hessian

Hessian is the matrix of second derivatives of the log-likelihood

```{r, echo = TRUE}
vcov <- solve(fit$hessian)
se <- sqrt(diag(vcov))
se
```

------------------------------------------------------------------------

# Displaying results (2)

```{r, echo = TRUE}
plot(x, y)
curve(fit$par[1] + x * fit$par[1], from = 0, to = 20, add = TRUE)
```

------------------------------------------------------------------------

# Linear models in RTMB

```{r, echo = TRUE}
library(RTMB)

f <- function(parms) {
    y <- data$y
    x <- data$x
    a <- parms$a
    b <- parms$b
    logSigma <- parms$logSigma
    ADREPORT(exp(2*logSigma));
    nll = -sum(dnorm(y, a+b*x, exp(logSigma), TRUE))
    nll
}
```

----

```{r, echo = TRUE}
data <- list(y = y, x = x)
parameters <- list(a=0, b=0, logSigma=0)
obj <- MakeADFun(f, parameters, silent = TRUE)

obj$hessian <- TRUE
opt <- do.call("optim", obj)
# opt
# opt$hessian ## <-- FD hessian from optim
# obj$he()    ## <-- Analytical hessian
rep <- sdreport(obj)
summary(rep)
```

# Comparison of two groups (1)

$$
y \sim group
$$

$$
y \sim a + b*group
$$

```{r, echo = TRUE}
group <- rep(c(0, 1), each = n/2)
y <- 0.2 + 2 * group + rnorm(n, mean = 0, sd = 0.5) 

mod <- lm(y ~ as.factor(group))
```

------------------------------------------------------------------------

# `summary(mod)`

```{r}
summary(mod)
```

------------------------------------------------------------------------

# Comparison of two groups (2)

```{r, echo = TRUE}
plot(as.factor(group), y)

# Model
mean_val <- c(mod$coefficients[1], mod$coefficients[1]+mod$coefficients[2])
points(as.factor(c(0,1)), mean_val, pch = 16, col = c(2,3), cex = 2)
```

------------------------------------------------------------------------

# Multiple predictors (1)

One continuous and one categorical

$$
y \sim x1+ group
$$

$$
y \sim a + b1 * x1+ b2*group
$$

```{r, echo = TRUE}
y <- 0.2 + 0.3 * x + 2 * group + rnorm(n, mean = 0, sd = 0.5) 

mod <- lm(y ~ x + as.factor(group))
```

------------------------------------------------------------------------

# `summary(mod)`

```{r}
summary(mod)
```

------------------------------------------------------------------------

# Multiple predictors (2)

```{r, echo = TRUE}
plot(x, y, col = 1 + group, pch = 16)

# Fitted model
abline(a = mod$coefficients[1], b = mod$coefficients[2])
abline(a = mod$coefficients[1] + mod$coefficients[3], b = mod$coefficients[2], col = 2)
```

# Interaction (1)

One continuous and one categorical

For example: growth in high vs low acidity

$$
y \sim a + b1 * x1+ b2*x1*group
$$

```{r, echo = TRUE}
y <- 0.2 + 0.3 * x + x * group + rnorm(n, mean = 0, sd = 0.5) 

mod <- lm(y ~ x * as.factor(group))
mod <- lm(y ~ x + as.factor(group) + x * as.factor(group))
```

------------------------------------------------------------------------

# `summary(mod)` and no group effect!

```{r}
summary(mod)
```

------------------------------------------------------------------------

# Interaction (2)

```{r, echo = TRUE}
plot(x, y, col = 1 + group, pch = 16)

# Fitted model
abline(a = mod$coefficients[1], b = mod$coefficients[2])
abline(a = mod$coefficients[1]+mod$coefficients[3], b = mod$coefficients[2] + +mod$coefficients[4], col = 2)
```

------------------------------------------------------------------------

# Interaction (3)

```{r, echo = T}
library(palmerpenguins)
data(package = 'palmerpenguins')
```

------------------------------------------------------------------------

# Multiple continuous predictors (1)

$$
y \sim x1+x2
$$

$$
y \sim a + b1* x1+ b2*x2
$$

```{r, echo = TRUE}
x1 <- runif(n, 0, 20)
x2 <- runif(n, 5, 10)
y <- 0.2 + 0.3 * x1 + 2 * x2 + rnorm(n, mean = 0, sd = 0.5) 

mod <- lm(y ~ x1 + x2)
```

------------------------------------------------------------------------

# `summary(mod)`

```{r}
summary(mod)
```

------------------------------------------------------------------------

# Multiple continuous predictors (2)

```{r, echo = TRUE}
par(mfrow = c(2, 1), mar = c(4, 4, 0.5, 0.5))
plot(x1, y)
abline(a = mod$coefficients[1], b = mod$coefficients[2])

plot(x2, y)
abline(a = mod$coefficients[1], b = mod$coefficients[3])

```

------------------------------------------------------------------------

# Multiple continuous predictors (3)

What went wrong?

------------------------------------------------------------------------

# Multiple continuous predictors (4)

Have to fix covariates at arbitrary values!

------------------------------------------------------------------------

# Multiple continuous predictors (4)

```{r, echo = TRUE}
par(mfrow = c(2, 1), mar = c(4, 4, 0.5, 0.5))
plot(x1, y)
pred1 <- data.frame(x1 = x1, x2 = mean(x2))
pred1$y = predict(mod, pred1)
lines(pred1$x1, pred1$y)

plot(x2, y)
pred2 <- data.frame(x1 = mean(x1), x2 = x2)
pred2$y = predict(mod, pred2)
lines(pred2$x2, pred2$y)

```

------------------------------------------------------------------------

# Multiple predictors overview

It can be hard to visualize models with multiple predictors!

Packages like [marginaleffects](https://marginaleffects.com/) can help

------------------------------------------------------------------------

# Where models can fail

Collinearity (correlation between covariates)

```{r, echo = TRUE}
x1 <- runif(n, 0, 20)
rho = 1
x2 <- 2 + rho * x1 + (1-rho) * runif(n, 0, 20)
y <- 0.2 + 0.3 * x1 + 2 * x2 + rnorm(n, mean = 0, sd = 0.5) 

mod <- lm(y ~ x1 + x2)
```

------------------------------------------------------------------------

# What happens to SE as rho -\> 0

```{r}
rho_vec <- seq(0, 0.99, by = 0.01)
se_vec <- c()
for(i in 1:length(rho_vec)){
  set.seed(123)
  x2 <- 2 + rho_vec[i] * x1 + (1-rho_vec[i]) * runif(n, 0, 20)
  y <- 0.2 + 0.3 * x1 + 2 * x2 + rnorm(n, mean = 0, sd = 0.5) 
  mod <- lm(y ~ x1 + x2)
  se_vec[i] <- summary(mod)$coefficients[2,2]
}

plot(x = rho_vec, y = (se_vec), type = "l")

```

# Lack of independence

Collinearity (correlation between covariates)

```{r, echo = TRUE}
library(MASS)
x1 <- runif(1000, -1, 1)
Sigma = diag(1, nrow = length(x1))
Rho = 0.5
for(i in 1:length(x)){
  for(j in 1:length(x)){
    if(i != j){
      Sigma[i,j] = Rho^abs(i - j)
    }
  }
}

y = mvrnorm(n = 1, mu = 0.3 * x1, Sigma = Sigma)


mod <- lm(y ~ x1)
summary(mod)
```
